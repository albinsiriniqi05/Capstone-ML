{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "import csv\n",
    "from collections import Counter\n",
    "from sklearn import datasets, linear_model, model_selection, svm, tree\n",
    "import matplotlib.pylab as plt\n",
    "from tqdm import tqdm\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path, delimiter='|'):\n",
    "    data = []\n",
    "    with open(path, 'r') as file:\n",
    "        reader = csv.reader(file, delimiter=delimiter)\n",
    "        for row in reader:\n",
    "            data.append(row)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Albin\n",
      "[nltk_data]     Siriniqi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Albin Siriniqi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package universal_tagset to C:\\Users\\Albin\n",
      "[nltk_data]     Siriniqi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(text):\n",
    "    if len(text) > 30:\n",
    "        features = {}\n",
    "\n",
    "        features[\"punctuation_count\"] = len(re.findall('[^\\w\\s\\'\\\"]{3,}', text))\n",
    "\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        tokens_count = len(tokens)\n",
    "\n",
    "        features[\"tokens_count\"] = tokens_count\n",
    "\n",
    "        pos_tag = nltk.pos_tag(tokens, tagset='universal')\n",
    "        pos_tag = [i for i in pos_tag if (len(i[0]) != 1) or (i[1] == '.') or (i[0] == 'a')]\n",
    "\n",
    "        all_words = [i[0] for i in pos_tag if i[1] != '.']\n",
    "\n",
    "        length_sum = sum(len(w) for w in all_words)\n",
    "\n",
    "        features[\"average_word_length\"] = length_sum / len(all_words)\n",
    "\n",
    "        tag_frequency_dist = nltk.FreqDist(tag for (word, tag) in pos_tag)\n",
    "\n",
    "        features[\"tag_frequency_dist\"] = tag_frequency_dist\n",
    "\n",
    "        words = [w[0] for w in pos_tag if (w[1] in ['ADJECTIVE', 'ADVERB', 'NOUN', 'VERB', 'X']) and (len(w[0]) > 1) ]\n",
    "        words_count = len(words)\n",
    "\n",
    "        features[\"words_count\"] = words_count\n",
    "\n",
    "        features[\"unique_words_total\"] = len(Counter(words).keys())\n",
    "\n",
    "        uppercase_total = sum(1 for _ in filter(lambda str: str.isupper(), words))\n",
    "\n",
    "        features[\"uppercase_fraction\"] = uppercase_total/ words_count\n",
    "\n",
    "        tb = TextBlob(text)\n",
    "        features[\"polarity\"] = tb.polarity\n",
    "        features[\"subjectivity\"] = tb.subjectivity\n",
    "\n",
    "        return features\n",
    "\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_extract(data, label):\n",
    "    features = []\n",
    "\n",
    "    for idx in tqdm(range(len(data))):\n",
    "        article_features = extract(data[idx][1])\n",
    "        selected_features = {\n",
    "            'label': label,\n",
    "            'tokens_count': article_features[\"tokens_count\"],\n",
    "            'words_count': article_features[\"words_count\"], \n",
    "            'unique_words_count': article_features[\"unique_words_count\"], \n",
    "            'polarity_score': article_features[\"polarity\"], \n",
    "            'subjectivity_score': article_features[\"subjectivity\"],\n",
    "            'uppercase_fraction': article_features[\"uppercase_fraction\"],\n",
    "            'average_word_length': article_features[\"average_word_length\"],\n",
    "            'punctuation_count': article_feature['punctuation_count'],\n",
    "            'adjective_frequenct': article_features[\"tag_fd\"].freq(\"ADJECTIVE\"), \n",
    "            'adv_freq': article_features[\"tag_frequency_dist\"].freq(\"ADVERB\"),\n",
    "            'noun_freq': article_features[\"tag_frequency_dist\"].freq(\"NOUN\"),\n",
    "            'verb_freq': article_features[\"tag_frequency_dist\"].freq(\"VERB\"),\n",
    "            'other_freq': article_features[\"tag_frequency_dist\"].freq(\"X\"),\n",
    "            'vocabulary_uniqueness': article_features[\"unique_words_total\"]/article_features[\"words_count\"]\n",
    "        }\n",
    "\n",
    "        data_features.append(selected_features)\n",
    "\n",
    "    df = pd.DataFrame.from_dict(data_features)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Testing_dataset.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-21c6ee3c92f0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mreal_data_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Testing_dataset.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mreal_data_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Datasets/Training_dataset.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mfake_data_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Datasets/zlo_test_dataset.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mfake_data_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Datasets/zlo_train_dataset.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-b3baaf9bdbbe>\u001b[0m in \u001b[0;36mread_data\u001b[1;34m(path, delimiter)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mread_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'|'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m         \u001b[0mreader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdelimiter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Testing_dataset.csv'"
     ]
    }
   ],
   "source": [
    "real_data_test = read_data('Testing_dataset.csv')\n",
    "real_data_train = read_data('Datasets/Training_dataset.csv')\n",
    "fake_data_test = read_data('Datasets/zlo_test_dataset.csv')\n",
    "fake_data_train = read_data('Datasets/zlo_train_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'real_data_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-d425f6d580ea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_real_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataframe_extract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreal_data_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf_real_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataframe_extract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreal_data_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdf_fake_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataframe_extract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfake_data_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdf_fake_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataframe_extract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfake_data_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'real_data_test' is not defined"
     ]
    }
   ],
   "source": [
    "df_real_test = dataframe_extract(real_data_test, 1)\n",
    "df_real_train = dataframe_extract(real_data_train, 1)\n",
    "df_fake_test = dataframe_extract(fake_data_test, -1)\n",
    "df_fake_train = dataframe_extract(fake_data_train, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_real_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-e730b6310ff2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest_dataframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf_real_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdf_fake_test\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtrain_dataframe\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf_real_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_fake_train\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_real_test' is not defined"
     ]
    }
   ],
   "source": [
    "test_dataframe = pd.concat([df_real_test,df_fake_test])\n",
    "train_dataframe= pd.concat([df_real_train, df_fake_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-cade4c7346df>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdataframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'test_df' is not defined"
     ]
    }
   ],
   "source": [
    "dataframe = pd.concat([test_df, train_df], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_dataframe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-c77b26f1d634>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_dataframe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtraining_dataframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_dataframe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtestX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtesting_dataframe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtesting_dataframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtestY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtesting_dataframe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'training_dataframe' is not defined"
     ]
    }
   ],
   "source": [
    "X = np.array(training_dataframe[training_dataframe.columns[1:]])\n",
    "Y = np.array(training_dataframe['label'])\n",
    "\n",
    "testX = np.array(testing_dataframe[testing_dataframe.columns[1:]])\n",
    "testY = np.array(testing_dataframe['label'])\n",
    "\n",
    "L = 100\n",
    "K = np.ones(Y.shape)\n",
    "\n",
    "J = []\n",
    "\n",
    "Y_scores = 0\n",
    "train_errors = []\n",
    "train_loss = []\n",
    "\n",
    "test_errors = []\n",
    "testY_scores = 0\n",
    "\n",
    "\n",
    "complete_scores = []\n",
    "\n",
    "\n",
    "for l in range(L):\n",
    "    f_t = tree.DecisionTreeClassifier(max_depth=1).fit(X, Y, K)\n",
    "    error_t = ((f_t.predict(X) != Y) * K).mean()\n",
    "    if err_t > 0.5:\n",
    "        print(\"No weak classifier found\")\n",
    "        break\n",
    "    alpha_t = 0.5 * np.log((1 - error_t) / err_t)\n",
    "    J.append((alpha_t, f_t))\n",
    "    K *= np.exp(-alpha_t * Y * f_t.predict(X))\n",
    "\n",
    "    Y_scores += alpha_t * f_t.predict(X)\n",
    "    train_errors.append((np.sign(Y_scores) != Y).mean())\n",
    "    train_loss.append(np.exp(-Y_scores * Y).mean())\n",
    "\n",
    "    testY_scores += alpha_t * f_t.predict(testX)\n",
    "    test_errors.append((np.sign(testY_scores) != testY).mean())\n",
    "    complete_scores.append(testY_scores)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_errors, label=\"train\")\n",
    "plt.plot(test_errors, label=\"test\")\n",
    "plt.xlabel(\"AdaBoost\")\n",
    "plt.ylabel(\"Error rate\")\n",
    "_ = plt.legend()\n",
    "test_errors = np.array(test_errors)\n",
    "I = test_errors.argsort()\n",
    "total_smallest_errors = (test_errors[I[0]])\n",
    "total_errors = test_errors[-1]\n",
    "True_Positive = ((np.sign(complete_scores[I[0]]) == 1) & (testY == 1)).sum()\n",
    "False_Positive = ((np.sign(complete_scores[I[0]]) == 1) & (testY == -1)).sum()\n",
    "True_Negative = ((np.sign(complete_scores[I[0]]) == -1) & (testY == -1)).sum()\n",
    "False_Negative = ((np.sign(complete_scores[I[0]]) == -1) & (testY == 1)).sum()\n",
    "\n",
    "print(\"Size of test datasets: %.2f\" % (True_Positive + False_Positive + False_Negative + True_Negative))\n",
    "print(\"True positives total: %.2f \" % True_Positive)\n",
    "print(\"False positives total: %.2f \" % False_Positive)\n",
    "print(\"True negatives total: %.2f \" % True_Negative)\n",
    "print(\"False negatives total: %.2f \" % False_Negative)\n",
    "print(\"Error: %.3f \" % total_errors)\n",
    "print(\"Smallest error: %.3f \" % total_smallest_errors)\n",
    "print(\"Train error: %.3f \" % train_errors[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
